# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2014-2018, The Alibaba Group Holding Ltd.
# This file is distributed under the same license as the PyODPS package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyODPS 0.7.16\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-01-09 19:53+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/base-tables.rst:4
msgid "表"
msgstr "Tables"

#: ../../source/base-tables.rst:6
msgid "`表 <https://help.aliyun.com/document_detail/27819.html>`_ 是ODPS的数据存储单元。"
msgstr ""
"`Tables <https://www.alibabacloud.com/help/en/doc-detail/27819.htm>`_ are"
" the data storage unit in MaxCompute."

#: ../../source/base-tables.rst:9 ../../source/base-tables.rst:393
msgid "基本操作"
msgstr "Basic operations"

#: ../../source/base-tables.rst:11
msgid "我们可以用 ODPS 入口对象的 ``list_tables`` 来列出项目空间下的所有表。"
msgstr ""
"Use the ``list_tables`` method as the ODPS object to list all tables in a"
" project."

#: ../../source/base-tables.rst:13
msgid ""
"for table in o.list_tables():\n"
"    # 处理每个表"
msgstr ""
"for table in o.list_tables():\n"
"    # handle every table"

#: ../../source/base-tables.rst:18
msgid "通过调用 ``exist_table`` 来判断表是否存在。"
msgstr "Use ``exist_table`` to check whether the specified table exists."

#: ../../source/base-tables.rst:20
msgid "通过调用 ``get_table`` 来获取表。"
msgstr "Use ``get_table`` to obtain the specified table."

#: ../../source/base-tables.rst:22
msgid ""
">>> t = o.get_table('dual')\n"
">>> t.table_schema\n"
"odps.Schema {\n"
"  c_int_a                 bigint\n"
"  c_int_b                 bigint\n"
"  c_double_a              double\n"
"  c_double_b              double\n"
"  c_string_a              string\n"
"  c_string_b              string\n"
"  c_bool_a                boolean\n"
"  c_bool_b                boolean\n"
"  c_datetime_a            datetime\n"
"  c_datetime_b            datetime\n"
"}\n"
">>> t.lifecycle\n"
"-1\n"
">>> print(t.creation_time)\n"
"2014-05-15 14:58:43\n"
">>> t.is_virtual_view\n"
"False\n"
">>> t.size\n"
"1408\n"
">>> t.comment\n"
"'Dual Table Comment'\n"
">>> t.table_schema.columns\n"
"[<column c_int_a, type bigint>,\n"
" <column c_int_b, type bigint>,\n"
" <column c_double_a, type double>,\n"
" <column c_double_b, type double>,\n"
" <column c_string_a, type string>,\n"
" <column c_string_b, type string>,\n"
" <column c_bool_a, type boolean>,\n"
" <column c_bool_b, type boolean>,\n"
" <column c_datetime_a, type datetime>,\n"
" <column c_datetime_b, type datetime>]\n"
">>> t.table_schema['c_int_a']\n"
"<column c_int_a, type bigint>\n"
">>> t.table_schema['c_int_a'].comment\n"
"'Comment of column c_int_a'"
msgstr ""

#: ../../source/base-tables.rst:65
msgid "通过提供 ``project`` 参数，来跨project获取表。"
msgstr ""
"You can also provide the ``project`` parameter to obtain the specified "
"table from another project."

#: ../../source/base-tables.rst:67
msgid ">>> t = o.get_table('dual', project='other_project')"
msgstr ""

#: ../../source/base-tables.rst:75
msgid "创建表的Schema"
msgstr "Create the table schema"

#: ../../source/base-tables.rst:77
msgid "有两种方法来初始化。第一种方式通过表的列、以及可选的分区来初始化。"
msgstr ""
"You can initialize a table in two ways. First, you can use columns or "
"combination of columns and partitions columns to initialize the table."

#: ../../source/base-tables.rst:79
msgid ""
">>> from odps.models import TableSchema, Column, Partition\n"
">>> columns = [Column(name='num', type='bigint', comment='the column'),\n"
">>>            Column(name='num2', type='double', comment='the column2')]"
"\n"
">>> partitions = [Partition(name='pt', type='string', comment='the "
"partition')]\n"
">>> schema = TableSchema(columns=columns, partitions=partitions)\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]\n"
">>> schema.partitions\n"
"[<partition pt, type string>]\n"
">>> schema.names  # 获取非分区字段的字段名\n"
"['num', 'num2']\n"
">>> schema.types  # 获取非分区字段的字段类型\n"
"[bigint, double]"
msgstr ""
">>> from odps.models import TableSchema, Column, Partition\n"
">>> columns = [Column(name='num', type='bigint', comment='the column'),\n"
">>>            Column(name='num2', type='double', comment='the column2')]"
"\n"
">>> partitions = [Partition(name='pt', type='string', comment='the "
"partition')]\n"
">>> schema = TableSchema(columns=columns, partitions=partitions)\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]\n"
">>> schema.partitions\n"
"[<partition pt, type string>]\n"
">>> schema.names  # get column name of none-partition columns\n"
"['num', 'num2']\n"
">>> schema.types  # get column type of none-partition columns\n"
"[bigint, double]"

#: ../../source/base-tables.rst:98
msgid "第二种方法是使用 ``Schema.from_lists``，这种方法更容易调用，但显然无法直接设置列和分区的注释了。"
msgstr ""
"Second, you can use ``Schema.from_lists`` to initialize the table. This "
"method is easier, but you cannot directly set the comments of the columns"
" and the partitions."

#: ../../source/base-tables.rst:100
msgid ""
">>> schema = TableSchema.from_lists(['num', 'num2'], ['bigint', "
"'double'], ['pt'], ['string'])\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]"
msgstr ""

#: ../../source/base-tables.rst:109
msgid "创建表"
msgstr "Create tables"

#: ../../source/base-tables.rst:111
msgid "可以使用表 schema 来创建表，方法如下："
msgstr "You can use the table schema to create a table in the following way:"

#: ../../source/base-tables.rst:113
msgid ""
">>> table = o.create_table('my_new_table', schema)\n"
">>> table = o.create_table('my_new_table', schema, if_not_exists=True)  #"
" 只有不存在表时才创建\n"
">>> table = o.create_table('my_new_table', schema, lifecycle=7)  # 设置生命周期"
msgstr ""
">>> table = o.create_table('my_new_table', schema)\n"
">>> table = o.create_table('my_new_table', schema, if_not_exists=True)  #"
" create table only when the table does not exist\n"
">>> table = o.create_table('my_new_table', schema, lifecycle=7)  # "
"configure lifecycle of the table (in days)"

#: ../../source/base-tables.rst:120
msgid "更简单的方式是采用“字段名 字段类型”字符串来创建表，方法如下："
msgstr ""
"An easier way is to use a string in the structure of “field name field "
"type” to create the table, as shown in the following code:"

#: ../../source/base-tables.rst:122
msgid ""
">>> table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
">>> # 创建分区表可传入 (表字段列表, 分区字段列表)\n"
">>> table = o.create_table('my_new_table', ('num bigint, num2 double', "
"'pt string'), if_not_exists=True)"
msgstr ""
">>> table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
">>> # a tuple like (column list, partition list) can be passed to create "
"a partitioned table\n"
">>> table = o.create_table('my_new_table', ('num bigint, num2 double', "
"'pt string'), if_not_exists=True)"

#: ../../source/base-tables.rst:129
msgid ""
"在未经设置的情况下，创建表时，只允许使用 bigint、double、decimal、string、datetime、boolean、map 和 "
"array 类型。 如果你使用的是位于公共云上的服务，或者支持 tinyint、struct 等新类型，可以设置 "
"``options.sql.use_odps2_extension = True`` 打开这些类型的支持，示例如下："
msgstr ""
"By default, you can only use the bigint, double, decimal, string, "
"datetime, boolean, map and array types to create a table. If you use "
"public cloud services, you can set ``options.sql.use_odps2_extension = "
"True`` to enable more types such as tinyint and struct, as shown in the "
"following code:"

#: ../../source/base-tables.rst:133
msgid ""
">>> from odps import options\n"
">>> options.sql.use_odps2_extension = True\n"
">>> table = o.create_table('my_new_table', 'cat smallint, content "
"struct<title:varchar(100), body string>')"
msgstr ""

#: ../../source/base-tables.rst:141
msgid "同步表更新"
msgstr "Synchronize table updates"

#: ../../source/base-tables.rst:143
msgid "有时候，一个表可能被别的程序做了更新，比如schema有了变化。此时可以调用 ``reload`` 方法来更新。"
msgstr ""
"If a table has been updated by another program and has changes in the "
"schema, you can use ``reload`` to synchronize the update."

#: ../../source/base-tables.rst:145
msgid ">>> table.reload()"
msgstr ""

#: ../../source/base-tables.rst:151
msgid "行记录Record"
msgstr "Record"

#: ../../source/base-tables.rst:153
msgid "Record表示表的一行记录，我们在 Table 对象上调用 new_record 就可以创建一个新的 Record。"
msgstr ""
"A record is a row record in a table. You can use new_record of a table "
"object to create a new record."

#: ../../source/base-tables.rst:155
msgid ""
">>> t = o.get_table('mytable')\n"
">>> r = t.new_record(['val0', 'val1'])  # 值的个数必须等于表schema的字段数\n"
">>> r2 = t.new_record()  #  也可以不传入值\n"
">>> r2[0] = 'val0' # 可以通过偏移设置值\n"
">>> r2['field1'] = 'val1'  # 也可以通过字段名设置值\n"
">>> r2.field1 = 'val1'  # 通过属性设置值\n"
">>>\n"
">>> print(record[0])  # 取第0个位置的值\n"
">>> print(record['c_double_a'])  # 通过字段取值\n"
">>> print(record.c_double_a)  # 通过属性取值\n"
">>> print(record[0: 3])  # 切片操作\n"
">>> print(record[0, 2, 3])  # 取多个位置的值\n"
">>> print(record['c_int_a', 'c_double_a'])  # 通过多个字段取值"
msgstr ""
">>> t = o.get_table('mytable')\n"
">>> r = t.new_record(['val0', 'val1'])  # the number of values must be "
"the same with the number of columns in the schema\n"
">>> r2 = t.new_record()  # initializing without values is also acceptable"
"\n"
">>> r2[0] = 'val0' # values can be set via column indices\n"
">>> r2['field1'] = 'val1'  # values can also be set via column names\n"
">>> r2.field1 = 'val1'  # values can also be set via attributes\n"
">>>\n"
">>> print(record[0])  # get the value of Column 0\n"
">>> print(record['c_double_a'])  # get value via column name\n"
">>> print(record.c_double_a)  # get value via attributes\n"
">>> print(record[0: 3])  # slice over the column\n"
">>> print(record[0, 2, 3])  # get multiple values via indices\n"
">>> print(record['c_int_a', 'c_double_a'])  # get multiple values via "
"column names"

#: ../../source/base-tables.rst:175
msgid "获取表数据"
msgstr "Obtain table data"

#: ../../source/base-tables.rst:177
msgid "有若干种方法能够获取表数据。首先，如果只是查看每个表的开始的小于1万条数据，则可以使用 ``head`` 方法。"
msgstr ""
"You can obtain table data in different ways. First, you can use ``head`` "
"to retrieve the first 10,000 or fewer data items in each table."

#: ../../source/base-tables.rst:179
msgid ""
">>> t = o.get_table('dual')\n"
">>> for record in t.head(3):\n"
">>>     # 处理每个Record对象"
msgstr ""
">>> t = o.get_table('dual')\n"
">>> for record in t.head(3):\n"
">>>     # process every Record object"

#: ../../source/base-tables.rst:188
msgid "其次，在table上可以执行 ``open_reader`` 操作来打一个reader来读取数据。"
msgstr ""
"Then, use ``open_reader`` as the table object to open a reader and read "
"the data."

#: ../../source/base-tables.rst:190 ../../source/base-tables.rst:240
msgid "使用 with 表达式的写法："
msgstr "Open the reader using a WITH clause, as shown in the following code:"

#: ../../source/base-tables.rst:192
msgid ""
">>> with t.open_reader(partition='pt=test') as reader:\n"
">>>     count = reader.count\n"
">>>     for record in reader[5:10]:  # "
"可以执行多次，直到将count数量的record读完，这里可以改造成并行操作\n"
">>>         # 处理一条记录"
msgstr ""
">>> with t.open_reader(partition='pt=test') as reader:\n"
">>>     count = reader.count\n"
">>>     for record in reader[5:10]:  # This line can be executed many "
"times until all records are visited. Parallelism can also be introduced."
"\n"
">>>         # process one record"

#: ../../source/base-tables.rst:199
msgid "不使用 with 表达式的写法："
msgstr ""
"Open the reader without using a WITH clause, as shown in the following "
"code:"

#: ../../source/base-tables.rst:201
msgid ""
">>> reader = t.open_reader(partition='pt=test')\n"
">>> count = reader.count\n"
">>> for record in reader[5:10]:  # 可以执行多次，直到将count数量的record读完，这里可以改造成并行操作"
"\n"
">>>     # 处理一条记录"
msgstr ""
">>> reader = t.open_reader(partition='pt=test')\n"
">>> count = reader.count\n"
">>> for record in reader[5:10]:  # This line can be executed many times "
"until all records are visited. Parallelism can also be introduced.\n"
">>>     # process one record"

#: ../../source/base-tables.rst:208
msgid "更简单的调用方法是使用 ODPS 对象的 ``read_table`` 方法，例如"
msgstr ""
"An easier way is to use ``read_table`` as the ODPS object, as shown in "
"the following code:"

#: ../../source/base-tables.rst:210
msgid ""
">>> for record in o.read_table('test_table', partition='pt=test'):\n"
">>>     # 处理一条记录"
msgstr ""
">>> for record in o.read_table('test_table', partition='pt=test'):\n"
">>>     # process one record"

#: ../../source/base-tables.rst:215
msgid "直接读取成 Pandas DataFrame:"
msgstr "Read directly into Pandas DataFrames:"

#: ../../source/base-tables.rst:217
msgid ""
">>> with t.open_reader(partition='pt=test') as reader:\n"
">>>     pd_df = reader.to_pandas()"
msgstr ""

#: ../../source/base-tables.rst:224
msgid "利用多进程加速读取:"
msgstr "Accelerate data read using multiple processes:"

#: ../../source/base-tables.rst:226
msgid ""
">>> import multiprocessing\n"
">>> n_process = multiprocessing.cpu_count()\n"
">>> with t.open_reader(partition='pt=test') as reader:\n"
">>>     pd_df = reader.to_pandas(n_process=n_process)"
msgstr ""

#: ../../source/base-tables.rst:236
msgid "向表写数据"
msgstr "Write data to tables"

#: ../../source/base-tables.rst:238
msgid "类似于 ``open_reader``，table对象同样能执行 ``open_writer`` 来打开writer，并写数据。"
msgstr ""
"Similar to ``open_reader``, you can use ``open_writer`` as the table "
"object to open a writer and write data to the table."

#: ../../source/base-tables.rst:242
msgid ""
">>> with t.open_writer(partition='pt=test') as writer:\n"
">>>     records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # 这里records可以是可迭代对象\n"
">>>\n"
">>>     records = [t.new_record([111, 'aaa', True]),   # 也可以是Record对象\n"
">>>                t.new_record([222, 'bbb', False]),\n"
">>>                t.new_record([333, 'ccc', True]),\n"
">>>                t.new_record([444, '中文', False])]\n"
">>>     writer.write(records)\n"
">>>"
msgstr ""
">>> with t.open_writer(partition='pt=test') as writer:\n"
">>>     records = [[111, 'aaa', True],                 # a list can be "
"used here\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # records can also be iterable objects\n"
">>>\n"
">>>     records = [t.new_record([111, 'aaa', True]),   # a list with "
"records can also be used\n"
">>>                t.new_record([222, 'bbb', False]),\n"
">>>                t.new_record([333, 'ccc', True]),\n"
">>>                t.new_record([444, '中文', False])]\n"
">>>     writer.write(records)\n"
">>>"

#: ../../source/base-tables.rst:259
msgid "如果分区不存在，可以使用 ``create_partition`` 参数指定创建分区，如"
msgstr ""
"If the specified partition does not exist, use the ``create_partition`` "
"parameter to create a partition, as shown in the following code:"

#: ../../source/base-tables.rst:261
msgid ""
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # 这里records可以是可迭代对象"
msgstr ""
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],                 # a list can be "
"used here\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # records can also be iterable objects"

#: ../../source/base-tables.rst:270
msgid "更简单的写数据方法是使用 ODPS 对象的 write_table 方法，例如"
msgstr ""
"An easier way is to use write_table as the ODPS object to write data, as "
"shown in the following code:"

#: ../../source/base-tables.rst:272
msgid ""
">>> records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>            [222, 'bbb', False],\n"
">>>            [333, 'ccc', True],\n"
">>>            [444, '中文', False]]\n"
">>> o.write_table('test_table', records, partition='pt=test', "
"create_partition=True)"
msgstr ""
">>> records = [[111, 'aaa', True],                 # a list can be used "
"here\n"
">>>            [222, 'bbb', False],\n"
">>>            [333, 'ccc', True],\n"
">>>            [444, '中文', False]]\n"
">>> o.write_table('test_table', records, partition='pt=test', "
"create_partition=True)"

#: ../../source/base-tables.rst:282
msgid ""
"**注意**\\ ：每次调用 write_table，MaxCompute 都会在服务端生成一个文件。这一操作需要较大的时间开销， "
"同时过多的文件会降低后续的查询效率。因此，我们建议在使用 write_table 方法时，一次性写入多组数据， 或者传入一个 generator "
"对象。"
msgstr ""
"**Note**\\ ：Every time when ``write_table`` is invoked，MaxCompute "
"generates a new file on the server side, which is an expensive operation "
"that reduces the throughput drastically. What's more, too many files may "
"increase query time on that table. Hence we propose writing multiple "
"records or passing a Python generator object when calling "
"``write_table``."

#: ../../source/base-tables.rst:286
msgid ""
"write_table 写表时会追加到原有数据。如果需要覆盖数据，可以为 write_table 增加一个参数 "
"``overwrite=True`` （仅在 0.11.1以后支持），或者调用 table.truncate() / 删除分区后再建立分区。"
msgstr ""
"When calling ```write_table```, new data will be appended to existing "
"data. If you need to overwrite existing data, you can add an argument "
"``overwrite=True`` to ``write_table`` call when you are using PyODPS "
"later than 0.11.1, or call ``truncate`` on tables or partitions."

#: ../../source/base-tables.rst:289
msgid "使用多进程并行写数据："
msgstr "Use multiple processes to write records:"

#: ../../source/base-tables.rst:291
msgid ""
"每个进程写数据时共享同一个 session_id，但是有不同的 block_id，每个 block 对应服务端的一个文件， 最后主进程执行 "
"commit，完成数据上传。"
msgstr ""
"All processes share one single session_id but own different block_id, "
"each block_id represents a server-side file respectively. After all "
"writing done the main process commits and data are uploaded."

#: ../../source/base-tables.rst:294
msgid ""
"import random\n"
"from multiprocessing import Pool\n"
"from odps.tunnel import TableTunnel\n"
"\n"
"def write_records(session_id, block_id):\n"
"    # 使用指定的 id 创建 session\n"
"    local_session = tunnel.create_upload_session(table.name, "
"upload_id=session_id)\n"
"    # 创建 writer 时指定 block_id\n"
"    with local_session.open_record_writer(block_id) as writer:\n"
"        for i in range(5):\n"
"            # 生成数据并写入对应 block\n"
"            record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"            writer.write(record)\n"
"\n"
"if __name__ == '__main__':\n"
"    N_WORKERS = 3\n"
"\n"
"    table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"    tunnel = TableTunnel(o)\n"
"    upload_session = tunnel.create_upload_session(table.name)\n"
"\n"
"    # 每个进程使用同一个 session_id\n"
"    session_id = upload_session.id\n"
"\n"
"    pool = Pool(processes=N_WORKERS)\n"
"    futures = []\n"
"    block_ids = []\n"
"    for i in range(N_WORKERS):\n"
"        futures.append(pool.apply_async(write_records, (session_id, i)))\n"
"        block_ids.append(i)\n"
"    [f.get() for f in futures]\n"
"\n"
"    # 最后执行 commit，并指定所有 block\n"
"    upload_session.commit(block_ids)"
msgstr ""
"import random\n"
"from multiprocessing import Pool\n"
"from odps.tunnel import TableTunnel\n"
"\n"
"def write_records(session_id, block_id):\n"
"    # create sessions with given id from main process\n"
"    local_session = tunnel.create_upload_session(table.name, "
"upload_id=session_id)\n"
"    # specify block_id when creating writers\n"
"    with local_session.open_record_writer(block_id) as writer:\n"
"        for i in range(5):\n"
"            # generate data and write to corresponding blocks\n"
"            record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"            writer.write(record)\n"
"\n"
"if __name__ == '__main__':\n"
"    N_WORKERS = 3\n"
"\n"
"    table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"    tunnel = TableTunnel(o)\n"
"    upload_session = tunnel.create_upload_session(table.name)\n"
"\n"
"    # all processes share one single session_id\n"
"    session_id = upload_session.id\n"
"\n"
"    pool = Pool(processes=N_WORKERS)\n"
"    futures = []\n"
"    block_ids = []\n"
"    for i in range(N_WORKERS):\n"
"        futures.append(pool.apply_async(write_records, (session_id, i)))\n"
"        block_ids.append(i)\n"
"    [f.get() for f in futures]\n"
"\n"
"    # finally we call commit with all block_ids\n"
"    upload_session.commit(block_ids)"

#: ../../source/base-tables.rst:334
msgid "使用 Arrow 格式读写数据"
msgstr "Use Arrow format to read and write data"

#: ../../source/base-tables.rst:335
msgid ""
"`Apache Arrow <https://arrow.apache.org/>`_ "
"是一种跨语言的通用数据读写格式，支持在各种不同平台间进行数据交换。 自2021年起， MaxCompute 支持使用 Arrow "
"格式读取表数据，PyODPS 则从 0.11.2 版本开始支持该功能。具体地，如果在 Python 环境中安装 pyarrow 后，在调用 "
"``open_reader`` 或者 ``open_writer`` 时增加 ``arrow=True`` 参数，即可读写 `Arrow "
"RecordBatch <https://arrow.apache.org/docs/python/data.html#record-"
"batches>`_ 。"
msgstr ""
"`Apache Arrow <https://arrow.apache.org/>`_ is a language-neutral format "
"supporting data exchange between different platforms. MaxCompute supports"
" reading and writing table data with Arrow format since 2021, and PyODPS "
"starts experimental support in 0.11.2. After installing pyarrow in your "
"Python environment, you can enable reading and writing with Arrow format "
"by adding ``arrow=True`` argument in ``open_reader`` or ``open_writer`` "
"calls to handle RecordBatch "
"<https://arrow.apache.org/docs/python/data.html#record-batches>`_ instead"
" of single records."

#: ../../source/base-tables.rst:340
msgid "按 RecordBatch 读取表内容："
msgstr "Read table content by record batches"

#: ../../source/base-tables.rst:342
msgid ""
">>> reader = t.open_reader(partition='pt=test', arrow=True)\n"
">>> count = reader.count\n"
">>> for batch in reader:  # 可以执行多次，直到将所有 RecordBatch 读完\n"
">>>     # 处理一个 RecordBatch，例如转换为 Pandas\n"
">>>     print(batch.to_pandas())"
msgstr ""
">>> reader = t.open_reader(partition='pt=test', arrow=True)\n"
">>> count = reader.count\n"
">>> for batch in reader:  # This line can be executed many times until "
"all record batches are visited.\n"
">>>     # process one RecordBatch, for instance, convert to Pandas\n"
">>>     print(batch.to_pandas())"

#: ../../source/base-tables.rst:350
msgid "写入 RecordBatch："
msgstr "Write record batches"

#: ../../source/base-tables.rst:352
msgid ""
">>> import pandas as pd\n"
">>> import pyarrow as pa\n"
">>>\n"
">>> with t.open_writer(partition='pt=test', create_partition=True, "
"arrow=True) as writer:\n"
">>>     records = [[111, 'aaa', True],\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     df = pd.DataFrame(records, columns=[\"int_val\", \"str_val\", "
"\"bool_val\"])\n"
">>>     # 写入 RecordBatch\n"
">>>     batch = pa.RecordBatch.from_pandas(df)\n"
">>>     writer.write(batch)\n"
">>>     # 也可以直接写入 Pandas DataFrame\n"
">>>     writer.write(df)"
msgstr ""
">>> import pandas as pd\n"
">>> import pyarrow as pa\n"
">>>\n"
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     df = pd.DataFrame(records, columns=[\"int_val\", \"str_val\", "
"\"bool_val\"])\n"
">>>     # write a RecordBatch\n"
">>>     batch = pa.RecordBatch.from_pandas(df)\n"
">>>     writer.write(batch)\n"
">>>     # Pandas DataFrame can also be used directly\n"
">>>     writer.write(df)"

#: ../../source/base-tables.rst:370
msgid "删除表"
msgstr "Delete tables"

#: ../../source/base-tables.rst:372
msgid ""
">>> o.delete_table('my_table_name', if_exists=True)  #  只有表存在时删除\n"
">>> t.drop()  # Table对象存在的时候可以直接执行drop函数"
msgstr ""
">>> o.delete_table('my_table_name', if_exists=True)  #  delete only when "
"the table exists\n"
">>> t.drop()  # call drop method of the Table object to delete directly"

#: ../../source/base-tables.rst:379
msgid "创建DataFrame"
msgstr "Create a DataFrame"

#: ../../source/base-tables.rst:381
msgid ""
"PyODPS提供了 :ref:`DataFrame框架 <df>` ，支持更方便地方式来查询和操作ODPS数据。 使用 ``to_df`` "
"方法，即可转化为 DataFrame 对象。"
msgstr ""
"PyODPS provides a :ref:`DataFrame framework <df>` to easily search and "
"operate MaxCompute data. You can use ``to_df`` to convert a table to a "
"DataFrame object."

#: ../../source/base-tables.rst:384
msgid ""
">>> table = o.get_table('my_table_name')\n"
">>> df = table.to_df()"
msgstr ""

#: ../../source/base-tables.rst:390
msgid "表分区"
msgstr "Table partitions"

#: ../../source/base-tables.rst:395
msgid "判断是否为分区表："
msgstr "Check if a table is partitioned:"

#: ../../source/base-tables.rst:397
#, python-format
msgid ""
">>> if table.table_schema.partitions:\n"
">>>     print('Table %s is partitioned.' % table.name)"
msgstr ""

#: ../../source/base-tables.rst:402
msgid "遍历表全部分区："
msgstr "Iterate through all the partitions in a table:"

#: ../../source/base-tables.rst:404
msgid ""
">>> for partition in table.partitions:\n"
">>>     print(partition.name)\n"
">>> for partition in table.iterate_partitions(spec='pt=test'):\n"
">>>     # 遍历二级分区"
msgstr ""
">>> for partition in table.partitions:\n"
">>>     print(partition.name)\n"
">>> for partition in table.iterate_partitions(spec='pt=test'):\n"
">>>     # iterate over secondary partitions"

#: ../../source/base-tables.rst:411
msgid "判断分区是否存在（该方法需要填写所有分区字段值）："
msgstr ""
"Check whether the specified partition exists, all field values should be "
"provided:"

#: ../../source/base-tables.rst:413
msgid ">>> table.exist_partition('pt=test,sub=2015')"
msgstr ""

#: ../../source/base-tables.rst:417
msgid "判断给定前缀的分区是否存在："
msgstr "Check whether partitions satisfying provided prefix exist:"

#: ../../source/base-tables.rst:419
msgid ""
">>> # 表 table 的分区字段依次为 pt, sub\n"
">>> table.exist_partitions('pt=test')"
msgstr ""
">>> # the order of partitions fields of table is pt, sub\n"
">>> table.exist_partitions('pt=test')"

#: ../../source/base-tables.rst:424
msgid "获取分区："
msgstr "Obtain the specified partition:"

#: ../../source/base-tables.rst:426
msgid ""
">>> partition = table.get_partition('pt=test')\n"
">>> print(partition.creation_time)\n"
"2015-11-18 22:22:27\n"
">>> partition.size\n"
"0"
msgstr ""

#: ../../source/base-tables.rst:436
msgid ""
"这里的\"分区\"指的不是分区字段而是所有分区字段均确定的分区定义对应的子表。如果某些分区未指定，那么这个分区定义可能对应多个子表， "
"``get_partition`` 时则不被 PyODPS 支持。此时，需要使用 ``iterate_partitions`` 分别处理每个分区。"
msgstr ""
"The word `partition` here refers to a partition specification that "
"specifies values of all partition columns which uniquely specifies a sub-"
"table, not partition columns. When some values of partition columns are "
"absent, the specification could represent multiple tables, and then "
"calling ``get_partitions`` with this specification is not supported in "
"PyODPS. You need to use ``iter_partitions`` to handle every partition "
"respectively."

#: ../../source/base-tables.rst:440
msgid "创建分区"
msgstr "Create partitions"

#: ../../source/base-tables.rst:442
msgid "下面的操作将创建一个分区："
msgstr "Code below will create a partition."

#: ../../source/base-tables.rst:444
msgid ">>> t.create_partition('pt=test', if_not_exists=True)  # 不存在的时候才创建"
msgstr ""
">>> t.create_partition('pt=test', if_not_exists=True)  # create only when"
" the partition does not exist"

#: ../../source/base-tables.rst:449
msgid "删除分区"
msgstr "Delete partitions"

#: ../../source/base-tables.rst:451
msgid "下面的操作将删除一个分区："
msgstr "Code below will delete a partition."

#: ../../source/base-tables.rst:453
msgid ""
">>> t.delete_partition('pt=test', if_exists=True)  # 存在的时候才删除\n"
">>> partition.drop()  # Partition对象存在的时候直接drop"
msgstr ""
">>> t.delete_partition('pt=test', if_exists=True)  # delete only when the"
" partition exists\n"
">>> partition.drop()  # delete directly via the drop method of the "
"partition object"

#: ../../source/base-tables.rst:459
msgid "获取值最大分区"
msgstr "Obtain the partition with maximal value:"

#: ../../source/base-tables.rst:460
msgid "很多时候你可能希望获取值最大的分区。例如，当以日期为分区值时，你可能希望获得日期最近的有数据的分区。"
msgstr ""
"Sometimes you want to get the partition with maximal value, for instance,"
" when dates are used as partition values, you may want to get the "
"partition with data and latest date."

#: ../../source/base-tables.rst:462
msgid "创建分区表并写入一些数据："
msgstr "Create a partitioned table and write some data."

#: ../../source/base-tables.rst:464
#, python-format
msgid ""
"t = o.create_table(\"test_multi_pt_table\", (\"col string\", \"pt1 "
"string, pt2 string\"))\n"
"for pt1, pt2 in ((\"a\", \"a\"), (\"a\", \"b\"), (\"b\", \"c\"), (\"b\", "
"\"d\")):\n"
"    o.write_table(\"test_multi_pt_table\", [[\"value\"]], "
"partition=\"pt1=%s,pt2=%s\" % (pt1, pt2))"
msgstr ""

#: ../../source/base-tables.rst:470
msgid "如果想要获得值最大的分区，可以使用下面的代码："
msgstr ""
"If you want to get the partition with maximal value, you can use code "
"below:"

#: ../../source/base-tables.rst:472
msgid ""
">>> part = t.get_max_partition()\n"
">>> part\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>\n"
">>> part.partition_spec[\"pt1\"]  # 获取某个分区字段的值\n"
"b"
msgstr ""
">>> part = t.get_max_partition()\n"
">>> part\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>\n"
">>> part.partition_spec[\"pt1\"]  # get value of certain partition field\n"
"b"

#: ../../source/base-tables.rst:480
msgid "如果只希望获得最新的分区而忽略分区内是否有数据，可以用"
msgstr ""
"If you want to get latest partition while ignore whether the partition "
"has data, you may use"

#: ../../source/base-tables.rst:482
msgid ""
">>> t.get_max_partition(skip_empty=False)\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>"
msgstr ""

#: ../../source/base-tables.rst:487
msgid "对于多级分区表，可以通过限定上级分区值来获得值最大的子分区，例如"
msgstr ""
"For tables with multiple partitions, you may specify the parent partition"
" specification to get child partition with maximal value, for instance,"

#: ../../source/base-tables.rst:489
msgid ""
">>> t.get_max_partition(\"pt1=a\")\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='a',pt2='b')>"
msgstr ""

#: ../../source/base-tables.rst:497
msgid "数据上传下载通道"
msgstr "Data upload and download channels"

#: ../../source/base-tables.rst:502
msgid ""
"不推荐直接使用tunnel接口（难用且复杂），推荐直接使用表的 :ref:`写 <table_write>` 和 :ref:`读 "
"<table_read>` 接口。"
msgstr ""
"If you just need to upload a small amount of data, we do not recommend "
"using table tunnel directly as there are more convenient :ref:`read "
"<table_read>` and :ref:`write <table_write>` methods which wraps table "
"tunnel invocations. However, when your data amount is large and "
"throughput is critical, you may try using table tunnel methods directly."

#: ../../source/base-tables.rst:506
msgid "ODPS Tunnel是ODPS的数据通道，用户可以通过Tunnel向ODPS中上传或者下载数据。"
msgstr ""
"MaxCompute Tunnel is the data channel of MaxCompute. You can use this to "
"upload data to or download data from MaxCompute."

#: ../../source/base-tables.rst:508
msgid "**注意**，如果安装了 **Cython**，在安装pyodps时会编译C代码，加速Tunnel的上传和下载。"
msgstr ""
"**Note**: In a **Cython** environment, PyODPS compiles C programs during "
"installation to increase the Tunnel upload and download speed when "
"required."

#: ../../source/base-tables.rst:511
msgid "上传"
msgstr "Upload"

#: ../../source/base-tables.rst:513
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(odps)\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"with upload_session.open_record_writer(0) as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"upload_session.commit([0])"
msgstr ""

#: ../../source/base-tables.rst:533
msgid "也可以使用流式上传的接口："
msgstr "You can also use stream-like upload interface:"

#: ../../source/base-tables.rst:535
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(odps)\n"
"upload_session = tunnel.create_stream_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"with upload_session.open_record_writer() as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"import pandas as pd\n"
"import pyarrow as pa\n"
"\n"
"with upload_session.open_arrow_writer() as writer:\n"
"    df = pd.DataFrame({\"name\": [\"test1\", \"test2\"], \"id\": "
"[\"id1\", \"id2\"]})\n"
"    batch = pa.RecordBatch.from_pandas(df)\n"
"    writer.write(batch)"
msgstr ""

#: ../../source/base-tables.rst:563
msgid "下载"
msgstr "Download"

#: ../../source/base-tables.rst:565
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(odps)\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"with download_session.open_record_reader(0, download_session.count) as "
"reader:\n"
"    for record in reader:\n"
"        # 处理每条记录\n"
"\n"
"with download_session.open_arrow_reader(0, download_session.count) as "
"reader:\n"
"    for batch in reader:\n"
"        # 处理每个 Arrow RecordBatch"
msgstr ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(odps)\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"with download_session.open_record_reader(0, download_session.count) as "
"reader:\n"
"    for record in reader:\n"
"        # process every record\n"
"with download_session.open_arrow_reader(0, download_session.count) as "
"reader:\n"
"    for batch in reader:\n"
"        # process every Arrow RecordBatch"

