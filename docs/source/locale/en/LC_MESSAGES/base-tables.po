# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2014-2018, The Alibaba Group Holding Ltd.
# This file is distributed under the same license as the PyODPS package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyODPS 0.7.16\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-02-26 09:47+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/base-tables.rst:4
msgid "表"
msgstr "Tables"

#: ../../source/base-tables.rst:6
msgid "`表 <https://help.aliyun.com/document_detail/27819.html>`_ 是ODPS的数据存储单元。"
msgstr ""
"`Tables <https://www.alibabacloud.com/help/en/doc-detail/27819.htm>`_ are"
" the data storage unit in MaxCompute."

#: ../../source/base-tables.rst:9 ../../source/base-tables.rst:456
msgid "基本操作"
msgstr "Basic operations"

#: ../../source/base-tables.rst:13
msgid ""
"本文档中的代码对 PyODPS 0.11.3 及后续版本有效。对早于 0.11.3 版本的 PyODPS，请使用 "
"``odps.models.Schema`` 代替 ``odps.models.TableSchema``，使用 ``schema`` 属性代替 "
"``table_schema`` 属性。"
msgstr ""
"Code in this document is only guaranteed to work under PyODPS 0.11.3 and "
"later versions. For PyODPS earlier than 0.11.3, please replace class "
"``odps.models.Schema`` with ``odps.models.TableSchema`` and ``schema`` "
"property with ``table_schema``."

#: ../../source/base-tables.rst:16
msgid "我们可以用 ODPS 入口对象的 ``list_tables`` 来列出项目空间下的所有表。"
msgstr ""
"Use the ``list_tables`` method as the ODPS object to list all tables in a"
" project."

#: ../../source/base-tables.rst:18
msgid ""
"for table in o.list_tables():\n"
"    print(table.name)"
msgstr ""

#: ../../source/base-tables.rst:23
msgid "可以通过 ``prefix`` 参数只列举给定前缀的表："
msgstr "You can list all tables with given prefix with ``prefix`` argument."

#: ../../source/base-tables.rst:25
msgid ""
"for table in o.list_tables(prefix=\"table_prefix\"):\n"
"    print(table.name)"
msgstr ""

#: ../../source/base-tables.rst:30
msgid ""
"通过该方法获取的 Table 对象不会自动加载表名以外的属性，此时获取这些属性（例如 ``table_schema`` 或者 "
"``creation_time``）可能导致额外的请求并造成额外的时间开销。如果需要在列举表的同时读取这些属性，在 PyODPS 0.11.5 "
"及后续版本中，可以为 ``list_tables`` 添加 ``extended=True`` 参数："
msgstr ""
"Table objects obtained with code above do not load properties other than "
"names. If you get properties like ``table_schema`` or ``creation_time``, "
"an extra remote call will be performed which may cause extra delay. If "
"you need to get these properties at the same time when listing tables, "
"you can add ``extended=True`` argument for ``list_tables`` in PyODPS "
"0.11.5 or later."

#: ../../source/base-tables.rst:34
msgid ""
"for table in o.list_tables(extended=True):\n"
"    print(table.name, table.creation_time)"
msgstr ""

#: ../../source/base-tables.rst:39
msgid "如果你需要按类型列举表，可以指定 ``type`` 参数。不同类型的表列举方法如下："
msgstr ""
"You can list tables of given type by specifying ``type`` argument. "
"Examples of listing different types of tables are shown below."

#: ../../source/base-tables.rst:41
msgid ""
"managed_tables = list(o.list_tables(type=\"managed_table\"))  # 列举内置表\n"
"external_tables = list(o.list_tables(type=\"external_table\"))  # 列举外表\n"
"virtual_views = list(o.list_tables(type=\"virtual_view\"))  # 列举视图\n"
"materialized_views = list(o.list_tables(type=\"materialized_view\"))  # "
"列举物化视图"
msgstr ""
"managed_tables = list(o.list_tables(type=\"managed_table\"))  # iterate "
"over MaxCompute-managed tables\n"
"external_tables = list(o.list_tables(type=\"external_table\"))  # iterate"
" over external tables\n"
"virtual_views = list(o.list_tables(type=\"virtual_view\"))  # iterate "
"over non-materialized views\n"
"materialized_views = list(o.list_tables(type=\"materialized_view\"))  # "
"iterate over materialized views"

#: ../../source/base-tables.rst:48
msgid "通过调用 ``exist_table`` 来判断表是否存在。"
msgstr "Use ``exist_table`` to check whether the specified table exists."

#: ../../source/base-tables.rst:50
msgid "o.exist_table('dual')"
msgstr ""

#: ../../source/base-tables.rst:54
msgid "通过调用 ``get_table`` 来获取表。"
msgstr "Use ``get_table`` to obtain the specified table."

#: ../../source/base-tables.rst:56
msgid ""
">>> t = o.get_table('dual')\n"
">>> t.table_schema\n"
"odps.Schema {\n"
"  c_int_a                 bigint\n"
"  c_int_b                 bigint\n"
"  c_double_a              double\n"
"  c_double_b              double\n"
"  c_string_a              string\n"
"  c_string_b              string\n"
"  c_bool_a                boolean\n"
"  c_bool_b                boolean\n"
"  c_datetime_a            datetime\n"
"  c_datetime_b            datetime\n"
"}\n"
">>> t.lifecycle\n"
"-1\n"
">>> print(t.creation_time)\n"
"2014-05-15 14:58:43\n"
">>> t.is_virtual_view\n"
"False\n"
">>> t.size\n"
"1408\n"
">>> t.comment\n"
"'Dual Table Comment'\n"
">>> t.table_schema.columns\n"
"[<column c_int_a, type bigint>,\n"
" <column c_int_b, type bigint>,\n"
" <column c_double_a, type double>,\n"
" <column c_double_b, type double>,\n"
" <column c_string_a, type string>,\n"
" <column c_string_b, type string>,\n"
" <column c_bool_a, type boolean>,\n"
" <column c_bool_b, type boolean>,\n"
" <column c_datetime_a, type datetime>,\n"
" <column c_datetime_b, type datetime>]\n"
">>> t.table_schema['c_int_a']\n"
"<column c_int_a, type bigint>\n"
">>> t.table_schema['c_int_a'].comment\n"
"'Comment of column c_int_a'"
msgstr ""

#: ../../source/base-tables.rst:99
msgid "通过提供 ``project`` 参数，来跨project获取表。"
msgstr ""
"You can also provide the ``project`` parameter to obtain the specified "
"table from another project."

#: ../../source/base-tables.rst:101
msgid ">>> t = o.get_table('dual', project='other_project')"
msgstr ""

#: ../../source/base-tables.rst:109
msgid "创建表的Schema"
msgstr "Create the table schema"

#: ../../source/base-tables.rst:111
msgid "有两种方法来初始化。第一种方式通过表的列、以及可选的分区来初始化。"
msgstr ""
"You can initialize a table in two ways. First, you can use columns or "
"combination of columns and partitions columns to initialize the table."

#: ../../source/base-tables.rst:113
msgid ""
">>> from odps.models import TableSchema, Column, Partition\n"
">>> columns = [Column(name='num', type='bigint', comment='the column'),\n"
">>>            Column(name='num2', type='double', comment='the column2')]"
"\n"
">>> partitions = [Partition(name='pt', type='string', comment='the "
"partition')]\n"
">>> schema = TableSchema(columns=columns, partitions=partitions)\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]\n"
">>> schema.partitions\n"
"[<partition pt, type string>]\n"
">>> schema.names  # 获取非分区字段的字段名\n"
"['num', 'num2']\n"
">>> schema.types  # 获取非分区字段的字段类型\n"
"[bigint, double]"
msgstr ""
">>> from odps.models import TableSchema, Column, Partition\n"
">>> columns = [Column(name='num', type='bigint', comment='the column'),\n"
">>>            Column(name='num2', type='double', comment='the column2')]"
"\n"
">>> partitions = [Partition(name='pt', type='string', comment='the "
"partition')]\n"
">>> schema = TableSchema(columns=columns, partitions=partitions)\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]\n"
">>> schema.partitions\n"
"[<partition pt, type string>]\n"
">>> schema.names  # get column name of none-partition columns\n"
"['num', 'num2']\n"
">>> schema.types  # get column type of none-partition columns\n"
"[bigint, double]"

#: ../../source/base-tables.rst:132
msgid "第二种方法是使用 ``Schema.from_lists``，这种方法更容易调用，但显然无法直接设置列和分区的注释了。"
msgstr ""
"Second, you can use ``Schema.from_lists`` to initialize the table. This "
"method is easier, but you cannot directly set the comments of the columns"
" and the partitions."

#: ../../source/base-tables.rst:134
msgid ""
">>> schema = TableSchema.from_lists(['num', 'num2'], ['bigint', "
"'double'], ['pt'], ['string'])\n"
">>> schema.columns\n"
"[<column num, type bigint>,\n"
" <column num2, type double>,\n"
" <partition pt, type string>]"
msgstr ""

#: ../../source/base-tables.rst:143
msgid "创建表"
msgstr "Create tables"

#: ../../source/base-tables.rst:145
msgid "可以使用表 schema 来创建表，方法如下："
msgstr "You can use the table schema to create a table in the following way:"

#: ../../source/base-tables.rst:147
msgid ""
">>> table = o.create_table('my_new_table', schema)\n"
">>> table = o.create_table('my_new_table', schema, if_not_exists=True)  #"
" 只有不存在表时才创建\n"
">>> table = o.create_table('my_new_table', schema, lifecycle=7)  # 设置生命周期"
msgstr ""
">>> table = o.create_table('my_new_table', schema)\n"
">>> table = o.create_table('my_new_table', schema, if_not_exists=True)  #"
" create table only when the table does not exist\n"
">>> table = o.create_table('my_new_table', schema, lifecycle=7)  # "
"configure lifecycle of the table (in days)"

#: ../../source/base-tables.rst:154
msgid "更简单的方式是采用“字段名 字段类型”字符串来创建表，方法如下："
msgstr ""
"An easier way is to use a string in the structure of “field name field "
"type” to create the table, as shown in the following code:"

#: ../../source/base-tables.rst:156
msgid ""
">>> table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
">>> # 创建分区表可传入 (表字段列表, 分区字段列表)\n"
">>> table = o.create_table('my_new_table', ('num bigint, num2 double', "
"'pt string'), if_not_exists=True)"
msgstr ""
">>> table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
">>> # a tuple like (column list, partition list) can be passed to create "
"a partitioned table\n"
">>> table = o.create_table('my_new_table', ('num bigint, num2 double', "
"'pt string'), if_not_exists=True)"

#: ../../source/base-tables.rst:163
msgid ""
"在未经设置的情况下，创建表时，只允许使用 bigint、double、decimal、string、datetime、boolean、map 和 "
"array 类型。\\ 如果你使用的是位于公共云上的服务，或者支持 tinyint、struct 等新类型，可以设置 "
"``options.sql.use_odps2_extension = True`` 打开这些类型的支持，示例如下："
msgstr ""
"By default, you can only use the bigint, double, decimal, string, "
"datetime, boolean, map and array types to create a table. If you use "
"public cloud services, you can set ``options.sql.use_odps2_extension = "
"True`` to enable more types such as tinyint and struct, as shown in the "
"following code:"

#: ../../source/base-tables.rst:167
msgid ""
">>> from odps import options\n"
">>> options.sql.use_odps2_extension = True\n"
">>> table = o.create_table('my_new_table', 'cat smallint, content "
"struct<title:varchar(100), body string>')"
msgstr ""

#: ../../source/base-tables.rst:175
msgid "同步表更新"
msgstr "Synchronize table updates"

#: ../../source/base-tables.rst:177
msgid "有时候，一个表可能被别的程序做了更新，比如schema有了变化。此时可以调用 ``reload`` 方法来更新。"
msgstr ""
"If a table has been updated by another program and has changes in the "
"schema, you can use ``reload`` to synchronize the update."

#: ../../source/base-tables.rst:179
msgid ">>> table.reload()"
msgstr ""

#: ../../source/base-tables.rst:185
msgid "行记录Record"
msgstr "Record"

#: ../../source/base-tables.rst:187
msgid "Record表示表的一行记录，我们在 Table 对象上调用 new_record 就可以创建一个新的 Record。"
msgstr ""
"A record is a row record in a table. You can use new_record of a table "
"object to create a new record."

#: ../../source/base-tables.rst:189
msgid ""
">>> t = o.get_table('mytable')\n"
">>> r = t.new_record(['val0', 'val1'])  # 值的个数必须等于表schema的字段数\n"
">>> r2 = t.new_record()  #  也可以不传入值\n"
">>> r2[0] = 'val0' # 可以通过偏移设置值\n"
">>> r2['field1'] = 'val1'  # 也可以通过字段名设置值\n"
">>> r2.field1 = 'val1'  # 通过属性设置值\n"
">>>\n"
">>> print(record[0])  # 取第0个位置的值\n"
">>> print(record['c_double_a'])  # 通过字段取值\n"
">>> print(record.c_double_a)  # 通过属性取值\n"
">>> print(record[0: 3])  # 切片操作\n"
">>> print(record[0, 2, 3])  # 取多个位置的值\n"
">>> print(record['c_int_a', 'c_double_a'])  # 通过多个字段取值"
msgstr ""
">>> t = o.get_table('mytable')\n"
">>> r = t.new_record(['val0', 'val1'])  # the number of values must be "
"the same with the number of columns in the schema\n"
">>> r2 = t.new_record()  # initializing without values is also acceptable"
"\n"
">>> r2[0] = 'val0' # values can be set via column indices\n"
">>> r2['field1'] = 'val1'  # values can also be set via column names\n"
">>> r2.field1 = 'val1'  # values can also be set via attributes\n"
">>>\n"
">>> print(record[0])  # get the value of Column 0\n"
">>> print(record['c_double_a'])  # get value via column name\n"
">>> print(record.c_double_a)  # get value via attributes\n"
">>> print(record[0: 3])  # slice over the column\n"
">>> print(record[0, 2, 3])  # get multiple values via indices\n"
">>> print(record['c_int_a', 'c_double_a'])  # get multiple values via "
"column names"

#: ../../source/base-tables.rst:209
msgid "获取表数据"
msgstr "Obtain table data"

#: ../../source/base-tables.rst:211
msgid "有若干种方法能够获取表数据。首先，如果只是查看每个表的开始的小于1万条数据，则可以使用 ``head`` 方法。"
msgstr ""
"You can obtain table data in different ways. First, you can use ``head`` "
"to retrieve the first 10,000 or fewer data items in each table."

#: ../../source/base-tables.rst:213
msgid ""
">>> t = o.get_table('dual')\n"
">>> for record in t.head(3):\n"
">>>     # 处理每个Record对象"
msgstr ""
">>> t = o.get_table('dual')\n"
">>> for record in t.head(3):\n"
">>>     # process every Record object"

#: ../../source/base-tables.rst:222
msgid ""
"其次，在 table 实例上可以执行 ``open_reader`` 操作来打一个 reader 来读取数据。如果表为分区表，需要引入 "
"``partition`` 参数指定需要读取的分区。"
msgstr ""
"Then, use ``open_reader`` as the table object to open a reader and read "
"the data. If you need to read data from a partitioned table, you need to "
"add a ``partition`` argument to specify the partition to read."

#: ../../source/base-tables.rst:225
msgid "使用 with 表达式的写法，with 表达式会保证离开时关闭 reader："
msgstr ""
"Open the reader using a WITH clause, as shown in the following code. It "
"is ensured by with expression that the reader is closed once the with "
"block is exited."

#: ../../source/base-tables.rst:227
msgid ""
">>> with t.open_reader(partition='pt=test,pt2=test2') as reader:\n"
">>>     count = reader.count\n"
">>>     for record in reader[5:10]:  # "
"可以执行多次，直到将count数量的record读完，这里可以改造成并行操作\n"
">>>         # 处理一条记录"
msgstr ""
">>> with t.open_reader(partition='pt=test,pt2=test2') as reader:\n"
">>>     count = reader.count\n"
">>>     for record in reader[5:10]:  # This line can be executed many "
"times until all records are visited. Parallelism can also be introduced."
"\n"
">>>         # process one record"

#: ../../source/base-tables.rst:234
msgid "不使用 with 表达式的写法："
msgstr ""
"Open the reader without using a WITH clause, as shown in the following "
"code:"

#: ../../source/base-tables.rst:236
msgid ""
">>> reader = t.open_reader(partition='pt=test,pt2=test2')\n"
">>> count = reader.count\n"
">>> for record in reader[5:10]:  # 可以执行多次，直到将count数量的record读完，这里可以改造成并行操作"
"\n"
">>>     # 处理一条记录\n"
">>> reader.close()"
msgstr ""
">>> reader = t.open_reader(partition='pt=test,pt2=test2')\n"
">>> count = reader.count\n"
">>> for record in reader[5:10]:  # This line can be executed many times "
"until all records are visited. Parallelism can also be introduced.\n"
">>>     # process one record\n"
">>> reader.close()"

#: ../../source/base-tables.rst:244
msgid "更简单的调用方法是使用 ODPS 对象的 ``read_table`` 方法，例如"
msgstr ""
"An easier way is to use ``read_table`` as the ODPS object, as shown in "
"the following code:"

#: ../../source/base-tables.rst:246
msgid ""
">>> for record in o.read_table('test_table', "
"partition='pt=test,pt2=test2'):\n"
">>>     # 处理一条记录"
msgstr ""
">>> for record in o.read_table('test_table', "
"partition='pt=test,pt2=test2'):\n"
">>>     # process one record"

#: ../../source/base-tables.rst:251
msgid "直接读取成 Pandas DataFrame:"
msgstr "Read directly into Pandas DataFrames:"

#: ../../source/base-tables.rst:253
msgid ""
">>> with t.open_reader(partition='pt=test,pt2=test2') as reader:\n"
">>>     pd_df = reader.to_pandas()"
msgstr ""

#: ../../source/base-tables.rst:260
msgid "利用多进程加速读取:"
msgstr "Accelerate data read using multiple processes:"

#: ../../source/base-tables.rst:262
msgid ""
">>> import multiprocessing\n"
">>> n_process = multiprocessing.cpu_count()\n"
">>> with t.open_reader(partition='pt=test,pt2=test2') as reader:\n"
">>>     pd_df = reader.to_pandas(n_process=n_process)"
msgstr ""

#: ../../source/base-tables.rst:271
msgid ""
"``open_reader`` 或者 ``read_table`` 方法仅支持读取单个分区。如果需要读取多个分区的值，例如\\ 读取所有符合 "
"``dt>20230119`` 这样条件的分区，需要使用 ``iterate_partitions`` 方法，详见 :ref:`遍历表分区 "
"<iterate_partitions>` 章节。"
msgstr ""
"``open_reader`` or ``read_table`` only supports reading from one single "
"partition. If you need to read from multiple partitions, for instance, "
"partitions specified by the inequality ``dt>20230119``, you need to use "
"method ``iterate_partitions``. For more details please take a look at "
":ref:`iterating over table partitions <iterate_partitions>` section."

#: ../../source/base-tables.rst:278
msgid "向表写数据"
msgstr "Write data to tables"

#: ../../source/base-tables.rst:280
msgid ""
"类似于 ``open_reader``，table对象同样能执行 ``open_writer`` "
"来打开writer，并写数据。如果表为分区表，需要引入 ``partition`` 参数指定需要写入的分区。"
msgstr ""
"Similar to ``open_reader``, you can use ``open_writer`` as the table "
"object to open a writer and write data to the table. If the table to "
"write is partitioned, you need to add a ``partition`` argument to specify"
" the partition to write into."

#: ../../source/base-tables.rst:283
msgid "使用 with 表达式的写法，with 表达式会保证离开时关闭 writer 并提交所有数据："
msgstr ""
"Open the reader using a WITH clause, as shown in the following code. It "
"is ensured by with expression that the writer is closed once the with "
"block is exited and all written data are committed."

#: ../../source/base-tables.rst:285
msgid ""
">>> with t.open_writer(partition='pt=test') as writer:\n"
">>>     records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # 这里records可以是可迭代对象\n"
">>>\n"
">>>     records = [t.new_record([111, 'aaa', True]),   # 也可以是Record对象\n"
">>>                t.new_record([222, 'bbb', False]),\n"
">>>                t.new_record([333, 'ccc', True]),\n"
">>>                t.new_record([444, '中文', False])]\n"
">>>     writer.write(records)\n"
">>>"
msgstr ""
">>> with t.open_writer(partition='pt=test') as writer:\n"
">>>     records = [[111, 'aaa', True],                 # a list can be "
"used here\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # records can also be iterable objects\n"
">>>\n"
">>>     records = [t.new_record([111, 'aaa', True]),   # a list with "
"records can also be used\n"
">>>                t.new_record([222, 'bbb', False]),\n"
">>>                t.new_record([333, 'ccc', True]),\n"
">>>                t.new_record([444, '中文', False])]\n"
">>>     writer.write(records)\n"
">>>"

#: ../../source/base-tables.rst:302
msgid "如果分区不存在，可以使用 ``create_partition`` 参数指定创建分区，如"
msgstr ""
"If the specified partition does not exist, use the ``create_partition`` "
"parameter to create a partition, as shown in the following code:"

#: ../../source/base-tables.rst:304
msgid ""
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # 这里records可以是可迭代对象"
msgstr ""
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],                 # a list can be "
"used here\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     writer.write(records)  # records can also be iterable objects"

#: ../../source/base-tables.rst:313
msgid "更简单的写数据方法是使用 ODPS 对象的 write_table 方法，例如"
msgstr ""
"An easier way is to use write_table as the ODPS object to write data, as "
"shown in the following code:"

#: ../../source/base-tables.rst:315
msgid ""
">>> records = [[111, 'aaa', True],                 # 这里可以是list\n"
">>>            [222, 'bbb', False],\n"
">>>            [333, 'ccc', True],\n"
">>>            [444, '中文', False]]\n"
">>> o.write_table('test_table', records, partition='pt=test', "
"create_partition=True)"
msgstr ""
">>> records = [[111, 'aaa', True],                 # a list can be used "
"here\n"
">>>            [222, 'bbb', False],\n"
">>>            [333, 'ccc', True],\n"
">>>            [444, '中文', False]]\n"
">>> o.write_table('test_table', records, partition='pt=test', "
"create_partition=True)"

#: ../../source/base-tables.rst:325
msgid ""
"**注意**\\ ：每次调用 write_table，MaxCompute 都会在服务端生成一个文件。这一操作需要较大的时间开销，\\ "
"同时过多的文件会降低后续的查询效率。因此，我们建议在使用 write_table 方法时，一次性写入多组数据，\\ 或者传入一个 "
"generator 对象。"
msgstr ""
"**Note**\\ ：Every time when ``write_table`` is invoked，MaxCompute "
"generates a new file on the server side, which is an expensive operation "
"that reduces the throughput drastically. What's more, too many files may "
"increase query time on that table. Hence we propose writing multiple "
"records or passing a Python generator object when calling "
"``write_table``."

#: ../../source/base-tables.rst:329
msgid ""
"write_table 写表时会追加到原有数据。如果需要覆盖数据，可以为 write_table 增加一个参数 "
"``overwrite=True`` （仅在 0.11.1 以后支持），或者调用 table.truncate() / 删除分区后再建立分区。"
msgstr ""
"When calling ```write_table```, new data will be appended to existing "
"data. If you need to overwrite existing data, you can add an argument "
"``overwrite=True`` to ``write_table`` call when you are using PyODPS "
"later than 0.11.1, or call ``truncate`` on tables or partitions."

#: ../../source/base-tables.rst:332
msgid ""
"你可以使用多线程写入数据。从 PyODPS 0.11.6 开始，直接将 open_writer 创建的 Writer 对象分发到\\ "
"各个线程中即可完成多线程写入，写入时请注意不要关闭 writer，待所有数据写入完成后再关闭 writer。"
msgstr ""
"You can write data with multiple threads. Since PyODPS 0.11.6, simply "
"spawning ``writer`` objects created with ``open_writer`` method into "
"different threads and then data can be written in those threads. Note "
"that you shall not close writers until all data are written."

#: ../../source/base-tables.rst:335
msgid ""
"import random\n"
"# Python 2.7 请从三方库 futures 中 import ThreadPoolExecutor\n"
"from concurrent.futures import ThreadPoolExecutor\n"
"\n"
"def write_records(writer):\n"
"    for i in range(5):\n"
"        # 生成数据并写入\n"
"        record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"        writer.write(record)\n"
"\n"
"N_THREADS = 3\n"
"\n"
"# 此处省略入口对象 o 的创建过程\n"
"table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"\n"
"with table.open_writer() as writer:\n"
"    pool = ThreadPoolExecutor(N_THREADS)\n"
"    futures = []\n"
"    for i in range(N_THREADS):\n"
"        futures.append(pool.submit(write_records, writer))\n"
"    # 等待线程中的写入完成\n"
"    [f.result() for f in futures]"
msgstr ""
"import random\n"
"# for Python 2.7 please import ThreadPoolExecutor from\n"
"# third-party library `futures`\n"
"from concurrent.futures import ThreadPoolExecutor\n"
"\n"
"def write_records(writer):\n"
"    for i in range(5):\n"
"        # generate data and write to passed writers\n"
"        record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"        writer.write(record)\n"
"\n"
"N_THREADS = 3\n"
"\n"
"# creation of MaxCompute entry object o is omitted here\n"
"table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"\n"
"with table.open_writer() as writer:\n"
"    pool = ThreadPoolExecutor(N_THREADS)\n"
"    futures = []\n"
"    for i in range(N_THREADS):\n"
"        futures.append(pool.submit(write_records, writer))\n"
"    # wait for threaded calls to finish\n"
"    [f.result() for f in futures]"

#: ../../source/base-tables.rst:360
msgid ""
"你也可以使用多进程写入数据，以避免 Python GIL 带来的性能损失。从 PyODPS 0.11.6 开始，只需要将 open_writer "
"创建的 Writer 对象通过 multiprocessing 标准库传递到需要写入的子进程中即可写入。\\ "
"需要注意的是，与多线程的情形不同，你应当在每个子进程完成写入后关闭 writer，并在所有写入子进程退出后\\ 再关闭主进程 writer（或离开"
" with 语句块），以保证所有数据被提交。"
msgstr ""
"You can also write data with ``multiprocessing`` module in Python to "
"avoid performance loss from GIL. Since PyODPS 0.11.6, you can simply pass"
" ``writer`` object created with ``open_writer`` method into subprocess "
"functions with ``multiprocessing`` APIs. Note that different from the "
"multi threading case, you need to close writers in every subprocess once "
"writing is finished and close writer in the main process once writing in "
"all subprocesses is done to make sure all written data are committed."

#: ../../source/base-tables.rst:365
msgid ""
"import random\n"
"from multiprocessing import Pool\n"
"\n"
"def write_records(writer):\n"
"    for i in range(5):\n"
"        # 生成数据并写入\n"
"        record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"        writer.write(record)\n"
"    # 需要手动在每个子进程中关闭连接\n"
"    writer.close()\n"
"\n"
"# 如果在独立的 Python 代码文件中，需要判断是否代码按主模块执行\n"
"# 以防止下面的代码被 multiprocessing 反复执行\n"
"if __name__ == '__main__':\n"
"    N_WORKERS = 3\n"
"\n"
"    # 此处省略入口对象 o 的创建过程\n"
"    table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"\n"
"    with table.open_writer() as writer:\n"
"        pool = Pool(processes=N_WORKERS)\n"
"        futures = []\n"
"        for i in range(N_WORKERS):\n"
"            futures.append(pool.apply_async(write_records, (writer,)))\n"
"        # 等待子进程中的执行完成\n"
"        [f.get() for f in futures]"
msgstr ""
"import random\n"
"from multiprocessing import Pool\n"
"\n"
"def write_records(writer):\n"
"    for i in range(5):\n"
"        # generate data and write to passed writers\n"
"        record = table.new_record([random.randint(1, 100), "
"random.random()])\n"
"        writer.write(record)\n"
"    # need to close writers in every subprocess once writing is done\n"
"    writer.close()\n"
"\n"
"# need to judge if current code is executed as main module to make sure\n"
"# the code is not executed by multiprocessing repeatedly\n"
"if __name__ == '__main__':\n"
"    N_WORKERS = 3\n"
"\n"
"    # creation of MaxCompute entry object o is omitted here\n"
"    table = o.create_table('my_new_table', 'num bigint, num2 double', "
"if_not_exists=True)\n"
"\n"
"    with table.open_writer() as writer:\n"
"        pool = Pool(processes=N_WORKERS)\n"
"        futures = []\n"
"        for i in range(N_WORKERS):\n"
"            futures.append(pool.apply_async(write_records, (writer,)))\n"
"        # wait for subprocesses to finish\n"
"        [f.get() for f in futures]"

#: ../../source/base-tables.rst:397
msgid "使用 Arrow 格式读写数据"
msgstr "Use Arrow format to read and write data"

#: ../../source/base-tables.rst:398
msgid ""
"`Apache Arrow <https://arrow.apache.org/>`_ "
"是一种跨语言的通用数据读写格式，支持在各种不同平台间进行数据交换。\\ 自2021年起， MaxCompute 支持使用 Arrow "
"格式读取表数据，PyODPS 则从 0.11.2 版本开始支持该功能。具体地，如果在 Python 环境中安装 pyarrow 后，在调用 "
"``open_reader`` 或者 ``open_writer`` 时增加 ``arrow=True`` 参数，即可读写 `Arrow "
"RecordBatch <https://arrow.apache.org/docs/python/data.html#record-"
"batches>`_ 。"
msgstr ""
"`Apache Arrow <https://arrow.apache.org/>`_ is a language-neutral format "
"supporting data exchange between different platforms. MaxCompute supports"
" reading and writing table data with Arrow format since 2021, and PyODPS "
"starts experimental support in 0.11.2. After installing pyarrow in your "
"Python environment, you can enable reading and writing with Arrow format "
"by adding ``arrow=True`` argument in ``open_reader`` or ``open_writer`` "
"calls to handle RecordBatch "
"<https://arrow.apache.org/docs/python/data.html#record-batches>`_ instead"
" of single records."

#: ../../source/base-tables.rst:403
msgid "按 RecordBatch 读取表内容："
msgstr "Read table content by record batches"

#: ../../source/base-tables.rst:405
msgid ""
">>> reader = t.open_reader(partition='pt=test', arrow=True)\n"
">>> count = reader.count\n"
">>> for batch in reader:  # 可以执行多次，直到将所有 RecordBatch 读完\n"
">>>     # 处理一个 RecordBatch，例如转换为 Pandas\n"
">>>     print(batch.to_pandas())"
msgstr ""
">>> reader = t.open_reader(partition='pt=test', arrow=True)\n"
">>> count = reader.count\n"
">>> for batch in reader:  # This line can be executed many times until "
"all record batches are visited.\n"
">>>     # process one RecordBatch, for instance, convert to Pandas\n"
">>>     print(batch.to_pandas())"

#: ../../source/base-tables.rst:413
msgid "写入 RecordBatch："
msgstr "Write record batches"

#: ../../source/base-tables.rst:415
msgid ""
">>> import pandas as pd\n"
">>> import pyarrow as pa\n"
">>>\n"
">>> with t.open_writer(partition='pt=test', create_partition=True, "
"arrow=True) as writer:\n"
">>>     records = [[111, 'aaa', True],\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     df = pd.DataFrame(records, columns=[\"int_val\", \"str_val\", "
"\"bool_val\"])\n"
">>>     # 写入 RecordBatch\n"
">>>     batch = pa.RecordBatch.from_pandas(df)\n"
">>>     writer.write(batch)\n"
">>>     # 也可以直接写入 Pandas DataFrame\n"
">>>     writer.write(df)"
msgstr ""
">>> import pandas as pd\n"
">>> import pyarrow as pa\n"
">>>\n"
">>> with t.open_writer(partition='pt=test', create_partition=True) as "
"writer:\n"
">>>     records = [[111, 'aaa', True],\n"
">>>                [222, 'bbb', False],\n"
">>>                [333, 'ccc', True],\n"
">>>                [444, '中文', False]]\n"
">>>     df = pd.DataFrame(records, columns=[\"int_val\", \"str_val\", "
"\"bool_val\"])\n"
">>>     # write a RecordBatch\n"
">>>     batch = pa.RecordBatch.from_pandas(df)\n"
">>>     writer.write(batch)\n"
">>>     # Pandas DataFrame can also be used directly\n"
">>>     writer.write(df)"

#: ../../source/base-tables.rst:433
msgid "删除表"
msgstr "Delete tables"

#: ../../source/base-tables.rst:435
msgid ""
">>> o.delete_table('my_table_name', if_exists=True)  #  只有表存在时删除\n"
">>> t.drop()  # Table对象存在的时候可以直接执行drop函数"
msgstr ""
">>> o.delete_table('my_table_name', if_exists=True)  #  delete only when "
"the table exists\n"
">>> t.drop()  # call drop method of the Table object to delete directly"

#: ../../source/base-tables.rst:442
msgid "创建DataFrame"
msgstr "Create a DataFrame"

#: ../../source/base-tables.rst:444
msgid ""
"PyODPS提供了 :ref:`DataFrame框架 <df>` ，支持更方便地方式来查询和操作ODPS数据。 使用 ``to_df`` "
"方法，即可转化为 DataFrame 对象。"
msgstr ""
"PyODPS provides a :ref:`DataFrame framework <df>` to easily search and "
"operate MaxCompute data. You can use ``to_df`` to convert a table to a "
"DataFrame object."

#: ../../source/base-tables.rst:447
msgid ""
">>> table = o.get_table('my_table_name')\n"
">>> df = table.to_df()"
msgstr ""

#: ../../source/base-tables.rst:453
msgid "表分区"
msgstr "Table partitions"

#: ../../source/base-tables.rst:458
msgid "判断表是否为分区表："
msgstr "Check if a table is partitioned:"

#: ../../source/base-tables.rst:460
#, python-format
msgid ""
">>> if table.table_schema.partitions:\n"
">>>     print('Table %s is partitioned.' % table.name)"
msgstr ""

#: ../../source/base-tables.rst:465
msgid "判断分区是否存在（该方法需要填写所有分区字段值）："
msgstr ""
"Check whether the specified partition exists, all field values should be "
"provided:"

#: ../../source/base-tables.rst:467
msgid ">>> table.exist_partition('pt=test,sub=2015')"
msgstr ""

#: ../../source/base-tables.rst:471
msgid "判断给定前缀的分区是否存在："
msgstr "Check whether partitions satisfying provided prefix exist:"

#: ../../source/base-tables.rst:473
msgid ""
">>> # 表 table 的分区字段依次为 pt, sub\n"
">>> table.exist_partitions('pt=test')"
msgstr ""
">>> # the order of partitions fields of table is pt, sub\n"
">>> table.exist_partitions('pt=test')"

#: ../../source/base-tables.rst:478
msgid "获取一个分区的相关信息："
msgstr "Obtain information about one specified partition:"

#: ../../source/base-tables.rst:480
msgid ""
">>> partition = table.get_partition('pt=test')\n"
">>> print(partition.creation_time)\n"
"2015-11-18 22:22:27\n"
">>> partition.size\n"
"0"
msgstr ""

#: ../../source/base-tables.rst:490
msgid ""
"这里的\"分区\"指的不是分区字段而是所有分区字段均确定的分区定义对应的子表。如果某个分区字段对应多个值， 则相应地有多个子表，即多个分区。而 "
"``get_partition`` 只能获取一个分区的信息。因而，"
msgstr ""
"The word `partition` here refers to a partition specification that "
"specifies values of all partition columns which uniquely specifies a sub-"
"table, not partition columns. If one partition column is specified with "
"multiple values, it may refer to multiple sub-tables, or multiple "
"partitions. Meanwhile the method ``get_partition`` can only obtain "
"information of only one sub-table. Thus,"

#: ../../source/base-tables.rst:493
msgid ""
"如果某些分区未指定，那么这个分区定义可能对应多个子表，``get_partition`` 时则不被 PyODPS 支持。\\ 此时，需要使用 "
"``iterate_partitions`` 分别处理每个分区。"
msgstr ""
"When some values of partition columns are absent, the specification could"
" represent multiple tables, and then calling ``get_partitions`` with this"
" specification is not supported in PyODPS. You need to use "
"``iter_partitions`` to handle every partition respectively."

#: ../../source/base-tables.rst:495
msgid ""
"如果某个分区字段被定义多次，或者使用类似 ``pt>20210302`` 这样的非确定逻辑表达式，则无法使用 ``get_partition`` "
"获取分区。在此情况下，可以尝试使用 ``iterate_partitions`` 枚举每个分区。"
msgstr ""
"When some partition column is specified multiple times, or non-"
"deterministic logic expressions like ``pt>20210302`` is used, "
"``get_partition`` cannot be used to obtain partition information. In this"
" case, ``iterate_partitions`` might be used to iterate over all "
"partitions."

#: ../../source/base-tables.rst:499
msgid "创建分区"
msgstr "Create partitions"

#: ../../source/base-tables.rst:501
msgid "下面的操作将创建一个分区，如果分区存在将报错："
msgstr ""
"Code below will create a partition or raise an error if the partition "
"already exists."

#: ../../source/base-tables.rst:503
msgid ">>> t.create_partition('pt=test')"
msgstr ""

#: ../../source/base-tables.rst:507
msgid "下面的操作将创建一个分区，如果分区存在则跳过："
msgstr ""
"Code below will create a partition or do nothing if the partition already"
" exists."

#: ../../source/base-tables.rst:509
msgid ">>> t.create_partition('pt=test', if_not_exists=True)"
msgstr ""

#: ../../source/base-tables.rst:516
msgid "遍历表分区"
msgstr "Iterate through partitions"

#: ../../source/base-tables.rst:517
msgid "下面的操作将遍历表全部分区："
msgstr "Code below iterates through all the partitions in a table."

#: ../../source/base-tables.rst:519
msgid ""
">>> for partition in table.partitions:\n"
">>>     print(partition.name)"
msgstr ""

#: ../../source/base-tables.rst:524
msgid "如果要遍历部分分区值确定的分区，可以使用 ``iterate_partitions`` 方法。"
msgstr ""
"If you need to iterate through partitions with certain values of "
"partition fields fixed, you can use ``iterate_partitions`` method."

#: ../../source/base-tables.rst:526
msgid ""
">>> for partition in table.iterate_partitions(spec='pt=test'):\n"
">>>     print(partition.name)"
msgstr ""

#: ../../source/base-tables.rst:531
msgid ""
"自 PyODPS 0.11.3 开始，支持为 ``iterate_partitions`` 指定简单的逻辑表达式及通过逗号连接，\\ "
"每个子表达式均须满足的复合逻辑表达式。或运算符暂不支持。"
msgstr ""
"Since PyODPS 0.11.3, PyODPS supports using simple logic expressions or "
"logic expressions connected with commas which means combined conditions "
"when iterating through partitions. OR operator is not supported "
"currently."

#: ../../source/base-tables.rst:534
msgid ""
">>> for partition in table.iterate_partitions(spec='dt>20230119'):\n"
">>>     print(partition.name)"
msgstr ""

#: ../../source/base-tables.rst:541
msgid ""
"在 0.11.3 之前的版本中，``iterate_partitions`` 仅支持枚举前若干个分区等于相应值的情形。例如， "
"当表的分区字段按顺序分别为 pt1、pt2 和 pt3，那么 ``iterate_partitions`` 中的  ``spec`` 参数只能指定"
" ``pt1=xxx`` 或者 ``pt1=xxx,pt2=yyy`` 这样的形式。自 0.11.3 开始， "
"``iterate_partitions`` 支持更多枚举方式，但仍建议尽可能限定上一级分区以提高枚举的效率。"
msgstr ""
"Before 0.11.3, ``iterate_partitions`` only supports specifying partition "
"values for the first partition fields. For instance, when a table has 3 "
"partition fields, pt1, pt2 and pt3, ``spec`` argument of "
"``iterate_partitions`` can only accept values like ``pt1=xxx`` or "
"``pt1=xxx,pt2=yyy``. Since 0.11.3, ``iterate_partitions`` supports more "
"flexible forms of ``spec`` arguments. However, it is still recommended to"
" fix values of first partition fields to improve speed of iteration."

#: ../../source/base-tables.rst:547
msgid "删除分区"
msgstr "Delete partitions"

#: ../../source/base-tables.rst:549
msgid "下面的操作将删除一个分区："
msgstr "Code below will delete a partition."

#: ../../source/base-tables.rst:551
msgid ""
">>> t.delete_partition('pt=test', if_exists=True)  # 存在的时候才删除\n"
">>> partition.drop()  # Partition对象存在的时候直接drop"
msgstr ""
">>> t.delete_partition('pt=test', if_exists=True)  # delete only when the"
" partition exists\n"
">>> partition.drop()  # delete directly via the drop method of the "
"partition object"

#: ../../source/base-tables.rst:557
msgid "获取值最大分区"
msgstr "Obtain the partition with maximal value:"

#: ../../source/base-tables.rst:558
msgid "很多时候你可能希望获取值最大的分区。例如，当以日期为分区值时，你可能希望获得日期最近的有数据的分区。PyODPS 自 0.11.3 开始支持此功能。"
msgstr ""
"Sometimes you want to get the partition with maximal value, for instance,"
" when dates are used as partition values, you may want to get the "
"partition with data and latest date. PyODPS starts supporting this "
"function since 0.11.3."

#: ../../source/base-tables.rst:561
msgid "创建分区表并写入一些数据："
msgstr "Create a partitioned table and write some data."

#: ../../source/base-tables.rst:563
#, python-format
msgid ""
"t = o.create_table(\"test_multi_pt_table\", (\"col string\", \"pt1 "
"string, pt2 string\"))\n"
"for pt1, pt2 in ((\"a\", \"a\"), (\"a\", \"b\"), (\"b\", \"c\"), (\"b\", "
"\"d\")):\n"
"    o.write_table(\"test_multi_pt_table\", [[\"value\"]], "
"partition=\"pt1=%s,pt2=%s\" % (pt1, pt2))"
msgstr ""

#: ../../source/base-tables.rst:569
msgid "如果想要获得值最大的分区，可以使用下面的代码："
msgstr ""
"If you want to get the partition with maximal value, you can use code "
"below:"

#: ../../source/base-tables.rst:571
msgid ""
">>> part = t.get_max_partition()\n"
">>> part\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>\n"
">>> part.partition_spec[\"pt1\"]  # 获取某个分区字段的值\n"
"b"
msgstr ""
">>> part = t.get_max_partition()\n"
">>> part\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>\n"
">>> part.partition_spec[\"pt1\"]  # get value of certain partition field\n"
"b"

#: ../../source/base-tables.rst:579
msgid "如果只希望获得最新的分区而忽略分区内是否有数据，可以用"
msgstr ""
"If you want to get latest partition while ignore whether the partition "
"has data, you may use"

#: ../../source/base-tables.rst:581
msgid ""
">>> t.get_max_partition(skip_empty=False)\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='b',pt2='d')>"
msgstr ""

#: ../../source/base-tables.rst:586
msgid "对于多级分区表，可以通过限定上级分区值来获得值最大的子分区，例如"
msgstr ""
"For tables with multiple partitions, you may specify the parent partition"
" specification to get child partition with maximal value, for instance,"

#: ../../source/base-tables.rst:588
msgid ""
">>> t.get_max_partition(\"pt1=a\")\n"
"<Partition cupid_test_release.`test_multi_pt_table`(pt1='a',pt2='b')>"
msgstr ""

#: ../../source/base-tables.rst:596
msgid "数据上传下载通道"
msgstr "Data upload and download channels"

#: ../../source/base-tables.rst:599
msgid ""
"不推荐直接使用 Tunnel 接口，该接口较为低级，简单的表写入推荐直接使用 Tunnel 接口上实现的表 :ref:`写 "
"<table_write>` 和 :ref:`读 <table_read>` 接口，可靠性和易用性更高。 只有在分布式写表等复杂场景下有直接使用 "
"Tunnel 接口的需要。"
msgstr ""
"If you just need to upload a small amount of data, we do not recommend "
"using table tunnel directly for simple table reading and writing, as "
"there are more convenient :ref:`read <table_read>` and :ref:`write "
"<table_write>` methods which wraps table tunnel invocations. You might "
"use tunnel interfaces directly when writing tables distributedly or under"
" complicated scenarios."

#: ../../source/base-tables.rst:603
msgid "ODPS Tunnel 是 MaxCompute 的数据通道，用户可以通过 Tunnel 向 MaxCompute 中上传或者下载数据。"
msgstr ""
"MaxCompute Tunnel is the data channel of MaxCompute. You can use this to "
"upload data to or download data from MaxCompute."

#: ../../source/base-tables.rst:606
msgid "上传"
msgstr "Upload"

#: ../../source/base-tables.rst:608
msgid "分块上传接口"
msgstr "Block upload interface"

#: ../../source/base-tables.rst:609
msgid ""
"直接使用 Tunnel 分块接口上传时，需要首先使用表名和分区创建 Upload Session，此后从 Upload Session 创建 "
"Writer。每个 Upload Session 可多次调用 ``open_record_writer`` 方法创建多个 Writer，每个 "
"Writer 拥有一个 ``block_id`` 对应一个数据块。完成写入后，需要调用 Upload Session 上的 ``commit`` "
"方法并指定需要提交的数据块列表。"
msgstr ""
"When using block upload interface of MaxCompute tunnel, you need to "
"create an upload session and then create writers on the session. You may "
"call ``open_record_writer`` multiple times to create multiple writers. "
"Every writer need to be specified with a unique ``block_id``. After "
"writing, you need to call ``commit`` method of the upload session to "
"commit selected blocks as a list."

#: ../../source/base-tables.rst:614
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# 为 table 和 pt=test 分区创建 Upload Session\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# 创建 record writer 并指定需要写入的 block_id 为 0\n"
"with upload_session.open_record_writer(0) as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"# 提交刚才写入的 block 0。多个 block id 需要同时提交\n"
"# 需要在 with 代码块外 commit，否则数据未写入即 commit，会导致报错\n"
"upload_session.commit([0])"
msgstr ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# create an upload session for my_table and partition pt=test\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# create a record writer with block_id as 0\n"
"with upload_session.open_record_writer(0) as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"# submit block 0 which is just written. multiple blocks need to be "
"specified\n"
"#  on commit.\n"
"# need to commit outside with-block, or data will not be written before\n"
"#  commit, and an error will be raised.\n"
"upload_session.commit([0])"

#: ../../source/base-tables.rst:638
msgid ""
"需要注意的是，指定 block id 后，所创建的 Writer 为长连接，如果长时间不写入会导致连接关闭，并导致写入失败，\\ 该时间通常为 5"
" 分钟。如果你写入数据的间隔较大，建议生成一批数据后再通过 ``open_record_writer`` 接口创建 Writer "
"并按需写入数据。如果你只希望在单个 Writer 上通过 Tunnel 写入数据，可以考虑在调用 ``open_record_writer`` "
"时不指定 block id，此时创建的 Writer 在写入数据时将首先将数据缓存在本地，当 Writer 关闭或者缓存数据大于\\ "
"一定大小（默认为 20MB，可通过 ``options.tunnel.block_buffer_size`` "
"指定）时才会写入数据。写入数据后，\\ 需要先通过 Writer 上的 ``get_blocks_written`` 方法获得已经写入的 "
"block 列表，再进行提交。"
msgstr ""
"Note that writers created with ``open_record_writer`` establish long "
"connections which will be closed if no data are written in a long period "
"and writing fails. The gap is often 5 minutes. If you write data with "
"long gaps, it is recommended to cache data locally and create writers "
"with ``open_record_writer`` and then write data immediately on demand. "
"For simplicity, if you only want to upload data with one single writer "
"with tunnel interface, you can call ``open_record_writer`` without block "
"id and a buffered writer will be created. The buffered writer will cache "
"data locally. Submission will be performed on writer close or buffer size"
" exceeded certain limits, which is 20MB by default and can be configured "
"with ``options.tunnel.block_buffer_size``. After writing all data you "
"need to obtain all written blocks with ``get_blocks_written`` method."

#: ../../source/base-tables.rst:645
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# 为 table 和 pt=test 分区创建 Upload Session\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# 不指定 block id 以创建带缓存的 record writer\n"
"with upload_session.open_record_writer() as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"# 需要在 with 代码块外 commit，否则数据未写入即 commit，会导致报错\n"
"# 从 writer 获得已经写入的 block id 并提交\n"
"upload_session.commit(writer.get_blocks_written())"
msgstr ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# create an upload session for my_table and partition pt=test\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# create writer without block id to create a record writer with buffers\n"
"with upload_session.open_record_writer() as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)\n"
"\n"
"# need to commit outside with-block, or data will not be written before\n"
"#  commit, and an error will be raised.\n"
"# obtain block ids from the writer and then commit\n"
"upload_session.commit(writer.get_blocks_written())"

#: ../../source/base-tables.rst:671
msgid ""
"使用带缓存的 Writer 时，需要注意不能在同一 Upload Session 上开启多个带缓存 Writer 进行写入，\\ "
"否则可能导致冲突而使数据丢失。"
msgstr ""
"When using buffered writers, you need to avoid opening multiple writers "
"on a single upload session, or there might be collisions and data might "
"be lost."

#: ../../source/base-tables.rst:674
msgid ""
"如果你需要使用 Arrow 格式而不是 Record 格式进行上传，可以将 ``open_record_writer`` 替换为 "
"``open_arrow_writer``，并写入 Arrow RecordBatch / Arrow Table 或者 pandas "
"DataFrame。"
msgstr ""
"If you need to upload with arrow format instead of record format, you may"
" replace ``open_record_writer`` with ``open_arrow_writer`` and write "
"arrow RecordBatches, Tables or pandas DataFrames."

#: ../../source/base-tables.rst:677
msgid ""
"import pandas as pd\n"
"import pyarrow as pa\n"
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# 使用 open_arrow_writer 而不是 open_record_writer\n"
"with upload_session.open_arrow_writer(0) as writer:\n"
"    df = pd.DataFrame({\"name\": [\"test1\", \"test2\"], \"id\": "
"[\"id1\", \"id2\"]})\n"
"    batch = pa.RecordBatch.from_pandas(df)\n"
"    writer.write(batch)\n"
"\n"
"# 需要在 with 代码块外 commit，否则数据未写入即 commit，会导致报错\n"
"upload_session.commit([0])"
msgstr ""
"import pandas as pd\n"
"import pyarrow as pa\n"
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"upload_session = tunnel.create_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"# use open_arrow_writer instead of open_record_writer\n"
"with upload_session.open_arrow_writer(0) as writer:\n"
"    df = pd.DataFrame({\"name\": [\"test1\", \"test2\"], \"id\": "
"[\"id1\", \"id2\"]})\n"
"    batch = pa.RecordBatch.from_pandas(df)\n"
"    writer.write(batch)\n"
"\n"
"# need to commit outside with-block, or data will not be written before\n"
"# commit, and an error will be raised.\n"
"upload_session.commit([0])"

#: ../../source/base-tables.rst:697
msgid "本章节中所述所有 Writer 均非线程安全。你需要为每个线程单独创建 Writer。"
msgstr ""
"All writers described in this chapter are not thread safe. You need to "
"create separate writers for every thread."

#: ../../source/base-tables.rst:700
msgid "流式上传接口"
msgstr "Stream upload interface"

#: ../../source/base-tables.rst:701
msgid ""
"MaxCompute 提供了\\ `流式上传接口 <https://help.aliyun.com/zh/maxcompute/user-"
"guide/overview-of-streaming-data-channels>`_\\ 用于简化分布式服务开发成本。可以使用 "
"``create_stream_upload_session`` 方法创建专门的 Upload Session。\\ 此时，不需要为该 "
"Session 的 ``open_record_writer`` 提供 block id。"
msgstr ""
"MaxCompute provides `stream upload interface "
"<https://help.aliyun.com/zh/maxcompute/user-guide/overview-of-streaming-"
"data-channels>`_ to reduce development cost of distributed services. You "
"may use ``create_stream_upload_session`` to create special upload "
"sessions. Block ids are not needed for ``open_record_writer`` for this "
"session type."

#: ../../source/base-tables.rst:705
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"table = o.get_table('my_table')\n"
"\n"
"tunnel = TableTunnel(o)\n"
"upload_session = tunnel.create_stream_upload_session(table.name, "
"partition_spec='pt=test')\n"
"\n"
"with upload_session.open_record_writer() as writer:\n"
"    record = table.new_record()\n"
"    record[0] = 'test1'\n"
"    record[1] = 'id1'\n"
"    writer.write(record)\n"
"\n"
"    record = table.new_record(['test2', 'id2'])\n"
"    writer.write(record)"
msgstr ""

#: ../../source/base-tables.rst:724
msgid "下载"
msgstr "Download"

#: ../../source/base-tables.rst:726
msgid ""
"直接使用 Tunnel 接口下载数据时，需要首先使用表名和分区创建 Download Session，此后从 Download Session "
"创建 Reader。每个 Download Session 可多次调用 ``open_record_reader`` 方法创建多个 "
"Reader，每个 Reader 需要指定起始行号以及终止行号。行号从 0 开始，终止行号可指定为 Session 的 ``count`` "
"属性，\\ 为表或分区的总行数。"
msgstr ""
"When using download interface of MaxCompute tunnel, you need to create a "
"download session and then create readers on the session. You may call "
"``open_record_reader`` multiple times to create multiple readers. Every "
"reader need to be specified with a start row number and a stop row "
"number. reading, you need to call ``commit`` method of the upload session"
" to commit selected blocks as a list."

#: ../../source/base-tables.rst:731
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# 为 table 和 pt=test 分区创建 Download Session\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"# 创建 record reader 并指定需要读取的行范围\n"
"with download_session.open_record_reader(0, download_session.count) as "
"reader:\n"
"    for record in reader:\n"
"        # 处理每条记录"
msgstr ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(o)\n"
"# create a download session for my_table and partition pt=test\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"# create a record reader and specify row range to read\n"
"with download_session.open_record_reader(0, download_session.count) as "
"reader:\n"
"    for record in reader:\n"
"        # process every record"

#: ../../source/base-tables.rst:744
msgid ""
"你也可以通过使用 ``open_arrow_reader`` 而不是 ``open_record_reader`` 使读取的数据为 Arrow "
"格式而不是 Record 格式。"
msgstr ""
"You can download data with arrow format instead of record format by "
"calling ``open_arrow_reader`` instead of ``open_record_reader``."

#: ../../source/base-tables.rst:747
msgid ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(o)\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"with download_session.open_arrow_reader(0, download_session.count) as "
"reader:\n"
"    for batch in reader:\n"
"        # 处理每个 Arrow RecordBatch"
msgstr ""
"from odps.tunnel import TableTunnel\n"
"\n"
"tunnel = TableTunnel(o)\n"
"download_session = tunnel.create_download_session('my_table', "
"partition_spec='pt=test')\n"
"\n"
"with download_session.open_arrow_reader(0, download_session.count) as "
"reader:\n"
"    for batch in reader:\n"
"        # process every Arrow RecordBatch"

